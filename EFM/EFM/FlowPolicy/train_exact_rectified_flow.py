#!/usr/bin/env python3
"""
Exact RectifiedFlow Policy Training - Perfect Implementation
Uses the exact RectifiedFlow implementation from the ImageGeneration repository
for perfect straight-line flow matching in imitation learning.
"""

import sys
import os
import pathlib

# Add FlowPolicy to path
FLOWPOLICY_ROOT = pathlib.Path(__file__).parent / 'FlowPolicy' / 'FlowPolicy'
if FLOWPOLICY_ROOT.exists():
    sys.path.insert(0, str(FLOWPOLICY_ROOT))
    os.chdir(str(FLOWPOLICY_ROOT))

import hydra
from omegaconf import OmegaConf
import torch
import numpy as np
import zarr
from torch.utils.data import Dataset

# ---- Exact RectifiedFlow components ----
from flow_policy_3d.policy.flowpolicy import FlowPolicy
from flow_policy_3d.model.common.normalizer import LinearNormalizer
from flow_policy_3d.dataset.base_dataset import BaseDataset
from flow_policy_3d.sde_lib import RectifiedFlow
from flow_policy_3d.losses_rectified_flow import get_rectified_flow_loss_fn
from flow_policy_3d.common.pytorch_util import dict_apply

class ExactRectifiedFlowPolicy(FlowPolicy):
    """
    Exact RectifiedFlow Implementation for Perfect Imitation Learning
    
    This class uses the EXACT implementation from the RectifiedFlow ImageGeneration repository
    to ensure perfect straight-line flow matching for action prediction.
    """
    
    def __init__(self, use_reflow=False, reflow_t_schedule='uniform', reflow_loss='l2', **kwargs):
        super().__init__(**kwargs)
        
        # Exact RectifiedFlow configuration
        self.use_reflow = use_reflow
        self.reflow_t_schedule = reflow_t_schedule
        self.reflow_loss = reflow_loss
        
        # Create exact RectifiedFlow instance
        self.sde = RectifiedFlow(
            init_type='gaussian',
            noise_scale=1.0,
            reflow_flag=use_reflow,
            reflow_t_schedule=reflow_t_schedule,
            reflow_loss=reflow_loss,
            use_ode_sampler='rk45',
            sigma_var=0.0,
            ode_tol=1e-5,
            sample_N=getattr(self, 'num_inference_step', 1)
        )
        
        print("üéØ Exact RectifiedFlow Policy Implementation:")
        print(f"   ‚Ä¢ Perfect straight-line flows from noise to data")
        print(f"   ‚Ä¢ Exact loss function: target = data - noise")
        print(f"   ‚Ä¢ Exact Euler sampling with proper time scaling")
        print(f"   ‚Ä¢ Use reflow: {use_reflow}")
        print(f"   ‚Ä¢ Reflow t schedule: {reflow_t_schedule}")
        print(f"   ‚Ä¢ Reflow loss: {reflow_loss}")
    
    def predict_action(self, obs_dict):
        """
        Exact RectifiedFlow prediction using proper Euler sampling
        """
        # Normalize input
        nobs = self.normalizer.normalize(obs_dict)
        if not self.use_pc_color:
            nobs['point_cloud'] = nobs['point_cloud'][..., :3]
        
        value = next(iter(nobs.values()))
        B, To = value.shape[:2]
        T = self.horizon
        Da = self.action_dim
        device = self.device
        dtype = self.dtype

        # Handle conditioning (same as original)
        if self.obs_as_global_cond:
            this_nobs = dict_apply(nobs, lambda x: x[:,:To,...].reshape(-1,*x.shape[2:]))
            nobs_features = self.obs_encoder(this_nobs)
            
            if "cross_attention" in self.condition_type:
                global_cond = nobs_features.reshape(B, self.n_obs_steps, -1)
            else:
                global_cond = nobs_features.reshape(B, -1)
            
            cond_data = torch.zeros(size=(B, T, Da), device=device, dtype=dtype)
            cond_mask = torch.zeros_like(cond_data, dtype=torch.bool)
            local_cond = None
        else:
            this_nobs = dict_apply(nobs, lambda x: x[:,:To,...].reshape(-1,*x.shape[2:]))
            nobs_features = self.obs_encoder(this_nobs)
            nobs_features = nobs_features.reshape(B, To, -1)
            Do = nobs_features.shape[-1]
            
            cond_data = torch.zeros(size=(B, T, Da+Do), device=device, dtype=dtype)
            cond_mask = torch.zeros_like(cond_data, dtype=torch.bool)
            cond_data[:,:To,Da:] = nobs_features
            cond_mask[:,:To,Da:] = True
            global_cond = None
        
        # Initialize from noise using exact RectifiedFlow
        x = self.sde.get_z0(cond_data, train=False).to(device)
        
        # Exact Euler sampling from RectifiedFlow ImageGeneration
        eps = 1e-3
        dt = 1. / self.sde.sample_N
        
        for i in range(self.sde.sample_N):
            num_t = i / self.sde.sample_N * (self.sde.T - eps) + eps
            t = torch.ones(x.shape[0], device=device) * num_t
            
            # Get velocity prediction
            pred = self.model(x, t*999, local_cond=local_cond, global_cond=global_cond)
            
            # Apply exact sigma variance (from ImageGeneration sampling.py)
            sigma_t = self.sde.sigma_t(num_t)
            pred_sigma = pred + (sigma_t**2)/(2*(self.sde.noise_scale**2)*((1.-num_t)**2)) * (0.5 * num_t * (1.-num_t) * pred - 0.5 * (2.-num_t)*x.detach().clone())
            
            # Exact Euler step
            x = x.detach().clone() + pred_sigma * dt + sigma_t * np.sqrt(dt) * torch.randn_like(pred_sigma).to(device)
        
        # Apply conditioning
        x[cond_mask] = cond_data[cond_mask]
        
        # Extract actions
        naction_pred = x[...,:Da]
        action_pred = self.normalizer['action'].unnormalize(naction_pred)
        
        start = To - 1
        end = start + self.n_action_steps
        action = action_pred[:,start:end]
        
        return {
            'action': action,
            'action_pred': action_pred,
        }
    
    def compute_loss(self, batch):
        """
        Exact RectifiedFlow loss computation
        """
        # Normalize observations and actions
        nobs = self.normalizer.normalize(batch['obs'])
        nactions = self.normalizer['action'].normalize(batch['action'])
        
        print(f"DEBUG: nactions shape: {nactions.shape}")
        
        if not self.use_pc_color:
            nobs['point_cloud'] = nobs['point_cloud'][..., :3]
        
        batch_size = nactions.shape[0]
        
        # Handle conditioning
        if self.obs_as_global_cond:
            this_nobs = dict_apply(nobs, 
                lambda x: x[:,:self.n_obs_steps,...].reshape(-1,*x.shape[2:]))
            nobs_features = self.obs_encoder(this_nobs)
            
            if "cross_attention" in self.condition_type:
                global_cond = nobs_features.reshape(batch_size, self.n_obs_steps, -1)
            else:
                global_cond = nobs_features.reshape(batch_size, -1)
            
            self._current_global_cond = global_cond
            self._current_local_cond = None
        else:
            this_nobs = dict_apply(nobs, lambda x: x.reshape(-1, *x.shape[2:]))
            nobs_features = self.obs_encoder(this_nobs)
            nobs_features = nobs_features.reshape(batch_size, nactions.shape[1], -1)
            
            # For local conditioning, we need to handle it properly in the loss
            self._current_global_cond = None
            self._current_local_cond = nobs_features
        
        # Get exact RectifiedFlow loss function
        loss_fn = get_rectified_flow_loss_fn(self.sde, train=True, reduce_mean=True, eps=1e-3)
        
        # Create a proper model wrapper class that matches RectifiedFlow expectations
        class ModelWrapper(torch.nn.Module):
            def __init__(self, flowpolicy_model, local_cond, global_cond, obs_as_global_cond, full_batch_size):
                super().__init__()
                self.flowpolicy_model = flowpolicy_model
                self.local_cond = local_cond
                self.global_cond = global_cond
                self.obs_as_global_cond = obs_as_global_cond
                self.full_batch_size = full_batch_size
            
            def forward(self, x, t_labels):
                """
                Forward pass matching RectifiedFlow model signature
                x: perturbed data from RectifiedFlow
                t_labels: time labels [B] (scaled by 999)
                """
                print(f"DEBUG: Input x shape: {x.shape}, t_labels shape: {t_labels.shape}")
                
                # Get the actual batch size being processed
                current_batch_size = x.shape[0]
                
                # Slice conditioning to match current batch size
                if self.obs_as_global_cond and self.global_cond is not None:
                    global_cond_slice = self.global_cond[:current_batch_size]
                    local_cond_slice = None
                else:
                    global_cond_slice = None
                    if self.local_cond is not None:
                        local_cond_slice = self.local_cond[:current_batch_size]
                    else:
                        local_cond_slice = None
                
                # Handle tensor shape conversion for FlowPolicy
                if len(x.shape) == 2:
                    # The model is being called with flattened samples
                    B = x.shape[0]
                    D = x.shape[1]  # This should be 7 for actions
                    T = 4  # horizon from config
                    
                    if D == 7:  # Pure action tensor
                        x = x.unsqueeze(1).repeat(1, T, 1)  # [B, D] -> [B, T, D]
                        print(f"DEBUG: Added time dimension: {x.shape}")
                    elif D == 28:  # 4*7 = flattened time*action 
                        x = x.reshape(B, T, 7)  # [B, T*D] -> [B, T, D]
                        print(f"DEBUG: Reshaped from flattened: {x.shape}")
                    else:
                        raise ValueError(f"Unexpected action dimension: {D}")
                
                # Let's try without transposing - keep [B, T, D] format
                print(f"DEBUG: x shape before model: {x.shape}")
                print(f"DEBUG: global_cond_slice shape: {global_cond_slice.shape if global_cond_slice is not None else None}")
                
                if self.obs_as_global_cond:
                    result = self.flowpolicy_model(x, t_labels, local_cond=None, global_cond=global_cond_slice)
                else:
                    # For local conditioning, concatenate observations with actions
                    B, T, Da = x.shape
                    Do = local_cond_slice.shape[-1]
                    x_with_obs = torch.cat([x, local_cond_slice], dim=-1)  # [B, T, Da+Do]
                    result = self.flowpolicy_model(x_with_obs, t_labels, local_cond=None, global_cond=None)
                    result = result[..., :Da]  # Keep only action dimensions [B, T, Da]
                
                print(f"DEBUG: Model result shape: {result.shape}")
                
                return result
        
        model_wrapper = ModelWrapper(
            self.model,
            self._current_local_cond, 
            self._current_global_cond,
            self.obs_as_global_cond,
            batch_size
        )
        
        # Compute exact RectifiedFlow loss
        loss = loss_fn(model_wrapper, nactions)
        
        loss_dict = {
            'exact_rectified_flow_loss': loss.item(),
        }
        
        return loss, loss_dict


# Helper function for dict operations
def dict_apply(data, func):
    """Apply function to all values in nested dict."""
    if isinstance(data, dict):
        return {k: dict_apply(v, func) for k, v in data.items()}
    else:
        return func(data)


# Import action normalizer
from action_normalizer_hybrid import array_to_stats, hybrid_action_normalizer_from_stat, SingleFieldLinearNormalizer

class YourDatasetAdapter(BaseDataset):
    """
    Dataset adapter for exact RectifiedFlow training
    """

    def __init__(self, 
                 zarr_path: str,
                 horizon: int = 4,
                 n_obs_steps: int = 2,
                 n_action_steps: int = 4,
                 train: bool = True,
                 split_ratio: float = 0.9,
                 **kwargs):
        self.zarr_path = zarr_path
        self.horizon = horizon
        self.n_obs_steps = n_obs_steps
        self.n_action_steps = n_action_steps
        self.train = train
        self.split_ratio = split_ratio

        # Load zarr data
        z = zarr.open(zarr_path, mode='r')
        self.pc_data = z['data']['point_cloud']     # (T, N, 6)
        self.state_data = z['data']['state']        # (T, 21)
        self.action_data = z['data']['action']      # (T, 7)
        self.episode_ends = z['meta']['episode_ends'][:]  # (E,)
        self.episode_starts = np.concatenate([[0], self.episode_ends[:-1]])

        # Valid sequence starts
        self.indices = []
        for start, end in zip(self.episode_starts, self.episode_ends):
            episode_len = end - start
            if episode_len >= horizon + n_obs_steps - 1:
                max_start = end - horizon - n_obs_steps + 1
                self.indices.extend(range(start, max_start + 1))
        self.indices = np.array(self.indices)

        # Split
        rng = np.random.RandomState(42)
        rng.shuffle(self.indices)
        n_train = int(len(self.indices) * split_ratio)
        self.indices = self.indices[:n_train] if train else self.indices[n_train:]
        self._rng = rng

        print(f"Dataset[{'train' if train else 'val'}]: {len(self.indices)} sequences")
        if len(self.indices) == 0:
            raise RuntimeError("No sequences available after split; check zarr and split_ratio.")

        # Fit action normalizer on sample
        sample_indices = rng.choice(self.indices, size=min(2000, len(self.indices)), replace=False)
        action_samples = []
        for idx in sample_indices:
            obs_start = idx
            obs_end = obs_start + self.n_obs_steps
            action_start = obs_end - 1
            action_end = action_start + self.horizon
            a = np.array(self.action_data[action_start:action_end])  # (horizon, 7)
            action_samples.append(a.reshape(-1, a.shape[-1]))
        all_action_sample = np.concatenate(action_samples, axis=0).astype(np.float32)

        # Fit hybrid normalizer
        stat = array_to_stats(all_action_sample)
        self.dp_action_norm: SingleFieldLinearNormalizer = hybrid_action_normalizer_from_stat(stat)
        self.normalizer = None

    def get_normalizer(self, mode='limits', **kwargs):
        """
        Fit FlowPolicy's LinearNormalizer on already normalized actions
        """
        sample_indices = self._rng.choice(self.indices, size=min(1000, len(self.indices)), replace=False)

        pc_samples, state_samples, action_samples = [], [], []
        for idx in sample_indices:
            obs_start = idx
            obs_end = obs_start + self.n_obs_steps

            pc = np.array(self.pc_data[obs_start:obs_end])        # (n_obs, N, 6)
            state = np.array(self.state_data[obs_start:obs_end])  # (n_obs, 21)

            action_start = obs_end - 1
            action_end = action_start + self.horizon
            actions_raw = np.array(self.action_data[action_start:action_end], dtype=np.float32)  # (horizon, 7)
            actions_hybrid = self.dp_action_norm.encode(actions_raw)

            pc_samples.append(pc.reshape(-1, pc.shape[-1]))
            state_samples.append(state.reshape(-1, state.shape[-1]))
            action_samples.append(actions_hybrid.reshape(-1, actions_hybrid.shape[-1]))

        all_pc = np.concatenate(pc_samples, axis=0).astype(np.float32)
        all_state = np.concatenate(state_samples, axis=0).astype(np.float32)
        all_action = np.concatenate(action_samples, axis=0).astype(np.float32)

        # Fit FlowPolicy's normalizer
        from flow_policy_3d.model.common.normalizer import LinearNormalizer
        normalizer = LinearNormalizer()
        normalizer.fit(data={'action': all_action, 'agent_pos': all_state, 'point_cloud': all_pc},
                       last_n_dims=1, mode=mode, **kwargs)
        self.normalizer = normalizer
        return normalizer

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        data_idx = self.indices[idx]
        # Observations
        obs_start = data_idx
        obs_end = obs_start + self.n_obs_steps
        point_cloud = np.array(self.pc_data[obs_start:obs_end]).astype(np.float32)  # (n_obs, N, 6)
        state = np.array(self.state_data[obs_start:obs_end]).astype(np.float32)     # (n_obs, 21)

        # Actions
        action_start = obs_end - 1
        action_end = action_start + self.horizon
        action_raw = np.array(self.action_data[action_start:action_end]).astype(np.float32)  # (horizon, 7)
        action_hybrid = self.dp_action_norm.encode(action_raw)  # Normalized for training

        return {
            'obs': {
                'point_cloud': torch.from_numpy(point_cloud).float(),
                'agent_pos': torch.from_numpy(state).float()
            },
            'action': torch.from_numpy(action_hybrid).float()
        }
    
    def get_validation_dataset(self):
        """Return validation dataset"""
        val_dataset = YourDatasetAdapter(
            zarr_path=self.zarr_path,
            horizon=self.horizon,
            n_obs_steps=self.n_obs_steps,
            n_action_steps=self.n_action_steps,
            train=False,
            split_ratio=self.split_ratio
        )
        # Share normalizers
        val_dataset.dp_action_norm = self.dp_action_norm
        val_dataset.normalizer = self.normalizer
        return val_dataset


def create_config():
    shape_meta = {
        'obs': {
            'point_cloud': {'shape': (8192, 6)},  # XYZ+RGB
            'agent_pos': {'shape': (21,)}
        },
        'action': {'shape': (7,)}
    }

    cfg = OmegaConf.create({
        'name': 'train_exact_rectified_flow_policy',
        'task_name': 'imitation_learning',
        'shape_meta': shape_meta,
        'exp_name': 'exact_rectified_flow',

        'horizon': 4,
        'n_obs_steps': 2,
        'n_action_steps': 4,
        'n_latency_steps': 0,
        'dataset_obs_steps': 2,
        'keypoint_visible_rate': 1.0,
        'obs_as_global_cond': True,

        'policy': {
            '_target_': '__main__.ExactRectifiedFlowPolicy',
            'use_reflow': True,  # Standard rectified flow training
            'reflow_t_schedule': 'uniform',
            'reflow_loss': 'l2',
            'use_point_crop': False,
            'condition_type': 'film',
            'use_down_condition': True,
            'use_mid_condition': True,
            'use_up_condition': True,
            'diffusion_step_embed_dim': 128,
            'down_dims': [256, 512, 1024],
            'crop_shape': [84, 84],
            'encoder_output_dim': 64,
            'horizon': 4,
            'kernel_size': 5,
            'n_action_steps': 4,
            'n_groups': 8,
            'n_obs_steps': 2,
            'obs_as_global_cond': True,
            'shape_meta': shape_meta,
            'use_pc_color': True,
            'pointnet_type': 'mlp',
            'pointcloud_encoder_cfg': {
                'in_channels': 6,
                'out_channels': 64,
                'use_layernorm': True
            },
            'Conditional_ConsistencyFM': {
                'eps': 1e-3,
                'num_segments': 2,
                'boundary': 1,
                'delta': 1e-3,
                'alpha': 1e-5,
                'num_inference_step': 1  # Exact RectifiedFlow uses 1 step for straight lines
            },
            'eta': 0.01
        },

        'task': {
            'dataset': {
                '_target_': '__main__.YourDatasetAdapter',
                'zarr_path': '/Riad/FlowPolicy/flowpolicy_training_data_with_demo_switching.zarr',
                'horizon': 4,
                'n_obs_steps': 2,
                'n_action_steps': 4
            },
            'env_runner': None
        },

        'dataloader': {
            'batch_size': 32,
            'shuffle': True,
            'num_workers': 16,
            'pin_memory': False,
            'drop_last': True
        },

        'val_dataloader': {
            'batch_size': 32,
            'shuffle': False,
            'num_workers': 16,
            'pin_memory': False,
            'drop_last': False
        },

        'optimizer': {
            '_target_': 'torch.optim.AdamW',
            'lr': 1e-4,  # Standard learning rate for RectifiedFlow
            'weight_decay': 1e-6
        },

        'training': {
            'seed': 42,
            'device': 'cuda',
            'num_epochs': 200,
            'gradient_accumulate_every': 1,
            'lr_scheduler': 'cosine',
            'lr_warmup_steps': 500,
            'use_ema': True,
            'rollout_every': 999,
            'checkpoint_every': 5,
            'val_every': 1,
            'sample_every': 5,
            'debug': False,
            'max_train_steps': None,
            'max_val_steps': None,
            'tqdm_interval_sec': 1.0,
            'resume': False,
            'early_stopping': {
                'enabled': True,
                'patience': 15,
                'min_delta': 1e-6,
                'monitor': 'val_loss'
            }
        },

        'ema': {
            '_target_': 'flow_policy_3d.model.flow.ema_model.EMAModel',
            'model': '???',
            'power': 0.75
        },

        'checkpoint': {
            'topk': {
                'k': 1,
                'mode': 'max',
                'monitor_key': 'test_mean_score'
            },
            'save_ckpt': True,
            'save_last_ckpt': True,
            'save_last_snapshot': False
        },

        'logging': {
            'project': 'exact_rectified_flow_policy',
            'group': 'imitation_learning',
            'name': 'exact_rectified_flow_run',
            'mode': 'disabled'
        }
    })
    return cfg

if __name__ == "__main__":
    # Import FlowPolicy training workspace
    from train import TrainFlowPolicyWorkspace

    # Create config
    cfg = create_config()

    # Create output directory
    output_dir = pathlib.Path("/Riad/FlowPolicy/exact_rectified_flow_training")
    output_dir.mkdir(exist_ok=True)

    print("üöÄ Starting Exact RectifiedFlow Policy Training...")
    print(f"üìÅ Output directory: {output_dir}")
    print(f"üéØ Training for {cfg.training.num_epochs} epochs")
    print(f"üìä Batch size: {cfg.dataloader.batch_size}")
    print(f"üîß Learning rate: {cfg.optimizer.lr}")
    print("üéØ EXACT RECTIFIED FLOW IMPLEMENTATION:")
    print("   ‚úÖ Perfect straight-line flows from noise to data")
    print("   ‚úÖ Exact loss: target = data - noise")
    print("   ‚úÖ Exact Euler sampling with proper time scaling t*999")
    print("   ‚úÖ Same implementation as ImageGeneration repository")
    print("   ‚úÖ No approximations or modifications")

    # Create workspace and run training
    workspace = TrainFlowPolicyWorkspace(cfg, output_dir=str(output_dir))
    workspace.run()